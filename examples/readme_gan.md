# GAN - Generative Adversarial Network implementation from scratch

## Overview
What kind of painting do you think Picasso would produce if he were still alive today?

Or, wouldn't it so cool if you can see another art by Andy Warhol?

GAN, or Generative Adversarial Network has potential to bring these dreams come close to reality.
GAN was invented by Mr. Ian Goodfellow in 2014.  You can refer to his original paper [Generative Adversarial Networks](https://arxiv.org/abs/1406.2661) if you are interested.

![Sample Images](https://github.com/hideyukiinada/ml/blob/master/assets/images/mnist-gan-example.jpg)

_Sample images generated by my GAN_

In short, GAN consists of two software objects:
* Generator
* Discriminator

Generator is responsible for generating a real-looking fake image from noise.
Discriminator is responsible for telling the fake objects from real objects.
During a training process, you alternate training for Discriminator and Generator.  As you iterate through many cycles, Generator learns to produce more real-looking fake images.
Now GAN became very popular and there are many variants.  Most notably [DCGAN](https://arxiv.org/abs/1511.06434) and [CycleGAN](https://junyanz.github.io/CycleGAN/).  The latter can generate images of zebras from horse images!

In this article, I would like to dive a little deeper and explaine how I implemented GAN to generate digit (number) looking images from scratch.

## Goal of this project
The objective of this project is to feed 60,000 digit images from [the MNIST database
of handwritten digits](http://yann.lecun.com/exdb/mnist/) into the system for training and make Generator produce images that look like digits.

## Architecture
![Sample Images](https://github.com/hideyukiinada/ml/blob/master/assets/images/gan_architecture.png)

The above diagram shows the overall architecture of the system.

### Structure of Discriminator
Discriminator consists of two-layer neural network with flattened 28 pixel by 28 pixel image as an input, 512 units on a hidden layer, 1 unit on the output layer:
```
discriminator = Model(num_input=28 * 28)
discriminator.add(Layer(512, activation=af.RELU))
discriminator.add(Layer(1, activation=af.SIGMOID))
```

### Structure of Generator
Generator consists of two parts connected:
  1. Two-layer neural network with a vector of 100 random numbers as an input, 512 units on a hidden layer, flattened 28 pixel by 28 pixel on the output layer
  2. Two-layer neural network from Discriminator
  
```
generator_discriminator = Model(num_input=100)
generator_discriminator.add(Layer(512, activation=af.LEAKY_RELU))
generator_discriminator.add(Layer(28 * 28, activation=af.SIGMOID))
generator_discriminator.add(Layer(512, activation=af.RELU))  # Needs to match discriminator
generator_discriminator.add(Layer(1, activation=af.SIGMOID))  # Needs to match discriminator
```
Generator needs Discriminator to judge if its output is real or fake, so the last two layers are shared with the Discriminator instance.  However, when the gradient of the error is back propagated during the training of generator, these two layers are locked so that weights and biases are not updated for these two layers.


## Training
### Training Discriminator
To train Discriminator, you feed the following data to it:
* Small batch of real MNIST digits as samples.  1 is fed for each of the sample as the ground-truth for these.
* Small batch of fake digits generated by the Generator.  0 is fed for each of the image telling Discriminator to consider each image as a fake.

![Sample Images](https://github.com/hideyukiinada/ml/blob/master/assets/images/discriminator_input.png)

### Training Generator
To train Generator, you feed the following data to it:
* Small batch of noise as samples.  1 is fed for each of the sample as the ground-truth for these. 

Generator converts this noise to what it thinks is a real-looking digit image. Whether the digit is real or fake is judged by Discriminator. What this means is that Generator will try to adjust itself so that it generates an image that looks real to Discriminator. 

## Issues to be addressed
Training GAN is sensitive to hyperparameters as well as weights initialization.  I made multiple attempts to generate diverse digits but the last attempt as shown in the top of this page still suffers from some numbers output more frequently than others (mode collapse).

## Source code
The main source code is available under the examples directory.  You need to run two scripts:
  * [Training script to train the model](https://github.com/hideyukiinada/ml/blob/master/examples/neural_network_mnist_gan_example)
  * [Prediction script to generate images using weights saved in training](https://github.com/hideyukiinada/ml/blob/master/examples/neural_network_mnist_gan_predict_example)

If you clone this repo, you should get other helper classes.

### Dependency on machine learning framework
This GAN is using the machine learning framework that I built from scratch using numpy, so there is no dependency on any other framework except for Keras to download MNIST data (see below).

### Dependencies on Keras to download MNIST data
The script uses Keras to download MNIST data, but does not rely on Keras for anything else, so if you already have MNIST data, you can replace the line below with your code to access your version of MNIST.
```
    (x, y), (x_test, y_test) = mnist.load_data()  # https://keras.io/datasets/
```

