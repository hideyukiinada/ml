# GAN - Generative Adversarial Network implementation from scratch

## Overview
What kind of painting do you think Picasso would produce if he were still alive today?
Or, wouldn't it so cool if you can see another art by Andy Warhol?
GAN, or Generative Adversarial Network has potential to bring these dreams come close to reality.
GAN was invented by Mr. Ian Goodfellow in 2014.  You can refer to his original paper [Generative Adversarial Networks](https://arxiv.org/abs/1406.2661) if you are interested.

![Sample Images](https://github.com/hideyukiinada/ml/blob/master/assets/images/mnist-gan-example.jpg)

_Sample images generated by my GAN_

In short, GAN consists of two software objects:
* Generator
* Discriminator

Generator is responsible for generating a real-looking fake image from noise.
Discriminator is responsible for telling the fake objects from real objects.
During a training process, you alternate training for Discriminator and Generator.  As you iterate through many cycles, Generator learns to produce more real-looking fake images.
Now GAN became very popular and there are many variants.  Most notably DCGAN and Cycle GAN.  The latter can generate images of zebras from horse images!

In this article, I would like to dive a little deeper and explaine how I implemented GAN to generate digit (number) looking images from scratch.

## Goal of this project
The objective of this project is feed 60,000 digit images from MNIST dataset into the system for training and make Generator produce images that look like digits.

## Architecture
![Sample Images](https://github.com/hideyukiinada/ml/blob/master/assets/images/gan_architecture.png)

The above diagram shows the overall architecture of the system.

### Structure of Discriminator
Discriminator consists of two-layer neural network with flattened 28 pixel by 28 pixel image as an input, 512 units on a hidden layer, 1 unit on the output layer:
```
discriminator = Model(num_input=28 * 28)
discriminator.add(Layer(512, activation=af.RELU))
discriminator.add(Layer(1, activation=af.SIGMOID))
```

### Structure of Generator
Generator consists of two parts connected:
  1. Two-layer neural network with a vector of 100 random numbers as an input, 512 units on a hidden layer, flattened 28 pixel by 28 pixel on the output layer
  2. Two-layer neural network from Discriminator
  
```
generator_discriminator = Model(num_input=100)
generator_discriminator.add(Layer(512, activation=af.LEAKY_RELU))
generator_discriminator.add(Layer(28 * 28, activation=af.SIGMOID))
generator_discriminator.add(Layer(512, activation=af.RELU))  # Needs to match discriminator
generator_discriminator.add(Layer(1, activation=af.SIGMOID))  # Needs to match discriminator
```
Generator needs Discriminator to judge if its output is real or fake, so the last two layers are shared with the Discriminator instance.  However, when the gradient of the error is back propagated during the training of generator, these two layers are locked so that weights and biases are not updated for these two layers.


## Training
### Training Discriminator
To train Discriminator, you feed the following data to it:
* Small batch of real MNIST digits as samples.  1 is fed for each of the sample as the ground-truth for these.
* Small batch of fake digits generated by the Generator.  0 is fed for each of the image telling Discriminator to consider each image as a fake.

### Training Generator
To train Generator, you feed the following data to it:
* Small batch of noise as samples.  1 is fed for each of the sample as the ground-truth for these. 

Generator converts this noise to what it thinks is a real-looking digit image. Whether the digit is real or fake is judged by Discriminator. What this means is that Generator will try to adjust itself so that it generates an image that looks real to Discriminator. 

## Source code
The main source code is available [here](https://github.com/hideyukiinada/ml/blob/master/examples/neural_network_mnist_gan_example).  If you clone this repo, you should get other helper classes.

### Dependencies on Keras to download MNIST data
The script uses Keras to download MNIST data, but does not rely on Keras for anything else, so if you already have MNIST data, you can replace the line below with your code to access your version of MNIST.
```
    (x, y), (x_test, y_test) = mnist.load_data()  # https://keras.io/datasets/
```

